{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RISE Camp Capstone Exercise\n",
    "\n",
    "To finish up RISE Camp this year, we're going to build an end-to-end application that takes advantage of some of the projects we've learned about in the last couple days. Those of you who attended last year's Camp will remember the Pong exercise that trained an RL policy in Ray and deployed it in Clipper. Today's version of Pong is more feature rich: We will take advantage of Flor's experiment tracking and WAVE's encryption capabilities, in addition to leveraging a new Clipper deployment model and RLlib's policies.\n",
    "\n",
    "As a brief overview, we will train three ML models in this exercise. The first two models will use [imitation learning](https://blog.statsbot.co/introduction-to-imitation-learning-32334c3b1e7a) to learn how to play Pong, and the third will train a reinforcement learning policy using RLlib and Ray. Flor will track the training processes for all three models. We will also encrypt each one of these models with WAVE and deploy & serve the models in Clipper.\n",
    "\n",
    "For those of you unfamiliar with imitation learning, the approach is simple. We previously recorded the actions of some humans playing Pong. Using this training data, we will build a logistic regression classifier that selects a paddle action (up, down, or stay) based on the current location, velocity, and trajectory of the ball.\n",
    "\n",
    "Finally, you'll play a game (or more!) against each of the three models. We'll aggregate the results to see which model performs the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python compatibility imports\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gym\n",
    "import pong_py\n",
    "import cloudpickle\n",
    "\n",
    "# ray imports\n",
    "import ray\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.agents import ppo\n",
    "\n",
    "import flor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Flor metadata for the notebook\n",
    "flor.setNotebookName('integration.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a separate notebook, we have set up a WAVE client and defined some helper functions that we'll use below. Feel free to look at the `wave-setup.ipynb` file in this directory if you'd like to dig in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run wave-setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call Wave helper function to create granting and receiving entities\n",
    "orgEntity, recipientEntity = createWaveEntities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imitation Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a look at our data. Run the cells below to load and inspect one of the two imitation learning datasets we'll use below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('imitation-small.csv')\n",
    "\n",
    "print(df.columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have many labeled actions (`up`, `down`, or `stop`), in addition to the corresponding state of the game -- the location of one user's paddle (we ignore the other user's actions), the location and velocity of the ball, and the previous location of the ball."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "First, we're going to define three Flor functions:\n",
    "\n",
    "* `preproc_imitation`: cleans the input data\n",
    "* `train_imitation_model`: trains an imitation learning model on the input data\n",
    "* `encrypt_model`: encrypts that model using Wave\n",
    "\n",
    "The preprocessing function reads an input CSV and converts the `up`, `down`, and `stay` labels into numerical values. It also normalizes the all the numerical values (the location of the controlled paddle, the location & velocity of the ball, and the previous location of the ball)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# PRE-DEFINED FLOR FUNCTION. CELL IS NOT EDITABLE.\n",
    "\n",
    "@flor.func\n",
    "def preproc_imitation(imitation_data, procd_imitation_data, **kwargs):\n",
    "    import pandas as pd\n",
    "    df_data = pd.read_csv(imitation_data)\n",
    "    \n",
    "    # drop the user column because we don't want to train on it\n",
    "    df_data = df_data.drop(labels=\"user\", axis=1)\n",
    "\n",
    "    # discretize the labels\n",
    "    def convert_label(label):\n",
    "        \"\"\"Convert labels into numeric values\"\"\"\n",
    "        if(label==\"down\"):\n",
    "            return 1\n",
    "        elif(label==\"up\"):\n",
    "            return 2\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # apply the discretization function defined above\n",
    "    df_data['label'] = df_data['label'].apply(convert_label)\n",
    "    \n",
    "    # normalize all the numerical values\n",
    "    df_data.loc[:, \"leftPaddle_y\":\"ball_y_prev\"] = df_data.loc[:, \"leftPaddle_y\":\"ball_y_prev\"]/500.0\n",
    "    \n",
    "    # write out the data to a file\n",
    "    df_data.to_json(procd_imitation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model training function takes a JSON blob of the cleaned data and fits a SciKit Learn logistic regression model to classify the action to take based on the input features. The model is pickled and dumped into a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# PRE-DEFINED FLOR FUNCTION. CELL IS NOT EDITABLE.\n",
    "\n",
    "@flor.func\n",
    "def train_imitation_model(procd_imitation_data, model, **kwargs):\n",
    "    import cloudpickle\n",
    "    import pandas as pd\n",
    "    from sklearn import linear_model\n",
    "    \n",
    "    # read in the data\n",
    "    df_data = pd.read_json(procd_imitation_data)\n",
    "    \n",
    "    # separate the desired label from the rest of the data\n",
    "    labels = df_data['label']\n",
    "    training_data= df_data.drop(['label'], axis=1)\n",
    "\n",
    "    # train the model\n",
    "    skmodel = linear_model.LogisticRegression()\n",
    "    skmodel.fit(training_data, labels)\n",
    "    \n",
    "    # write to a file\n",
    "    with open(model, 'wb') as f:\n",
    "        cloudpickle.dump(skmodel, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the `encrypt_model` function takes the model we trained above and a handle to a WAVE entity that has access to all models. It uses the WAVE entity to encrypt the model and serializes the ciphered model into a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# PRE-DEFINED FLOR FUNCTION. CELL IS NOT EDITABLE.\n",
    "\n",
    "@flor.func\n",
    "def encrypt_model(granting_entity, model, model_tag, encrypted_model, **kwargs):\n",
    "    import wave3 as wv\n",
    "    granting_entity = deserializeEntity(granting_entity)\n",
    "    \n",
    "    # read the model binary, so we can encrypt it\n",
    "    with open(model, 'rb') as f:\n",
    "        model = f.read()\n",
    "    \n",
    "    # NOTE: We are relying on a global handle to WAVE here. \n",
    "    # In practice, we would have to recreate this handle explicitly.\n",
    "    encrypt_response = wave.EncryptMessage(\n",
    "        wv.EncryptMessageParams(\n",
    "            # the namespace is the organization\n",
    "            namespace=granting_entity.hash,\n",
    "            resource=\"models/pong/\" + model_tag,\n",
    "            content=model))\n",
    "    \n",
    "    with open(encrypted_model, 'wb') as f:\n",
    "        f.write(encrypt_response.ciphertext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a Flor experiment called `pong-imitation` and link together the input data and the functions defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = 'imitation-small.csv'\n",
    "\n",
    "# get the small or large tag from the DATA_FILE variable\n",
    "model_tag = DATA_FILE.split('.')[0].split('-')[1]\n",
    "\n",
    "# where to serialize the deserialize the Wave entity\n",
    "ENTITY_FILE = 'org_entity.bin'\n",
    "\n",
    "with flor.Experiment('pong-imitation') as ex:\n",
    "    # load data into an artifact\n",
    "    imitation_data = ex.artifact(DATA_FILE, 'imitation_data')\n",
    "    \n",
    "    # call preprocessing function\n",
    "    do_preproc_imitation = ex.action(preproc_imitation, [imitation_data])\n",
    "    procd_imitation_data = ex.artifact('imitation_data.json', 'procd_imitation_data', do_preproc_imitation)\n",
    "    \n",
    "    # train the model \n",
    "    do_train_imitation_model = ex.action(train_imitation_model, [procd_imitation_data])\n",
    "    model = ex.artifact('model.pkl', 'model', do_train_imitation_model)\n",
    "    \n",
    "    model_tag = ex.literal(name='model_tag', v=model_tag)\n",
    "    \n",
    "    # serialize the wave entity, so we can track it as an artifact\n",
    "    serializeEntity(orgEntity, ENTITY_FILE)\n",
    "    granting_entity = ex.artifact(ENTITY_FILE, 'granting_entity')\n",
    "    \n",
    "    do_encrypt_model = ex.action(encrypt_model, [granting_entity, model, model_tag])\n",
    "    encrypted_model = ex.artifact('encrypted_model.bin', 'encrypted_model', do_encrypt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encrypted_model.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we will call `pull` on the encrypted model. As we saw in the Flor tutorial, this will run the whole pipeline and generate the model. The `utag` argument will be used to differentiate from different versions of the model we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encrypted_model.pull(utag=model_tag.v)\n",
    "model_location = encrypted_model.resolve_location()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model that we've trained, we are going to deploy it in Clipper. Run the cell below to start Clipper on your EC@ instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: YOU SHOULD ONLY RUN THIS CELL ONCE!\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "from clipper_admin import DockerContainerManager, ClipperConnection\n",
    "from clipper_admin.deployers import python as py_deployer\n",
    "from clipper_util.auth_deployer import auth_deploy_python_model\n",
    "\n",
    "# Make logging work correctly\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Start Clipper locally\n",
    "clipper_conn = ClipperConnection(DockerContainerManager())\n",
    "clipper_conn.stop_all()\n",
    "clipper_conn.start_clipper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we will attempt to deploy that model. You'll notice that we've wrapped the standard Clipper API with a special authenticated deployer that only deploys the model if it can decrypt it with the given recipient entity.\n",
    "\n",
    "Running the cell below _**should initially fail**_. This is because we haven't yet granted the `recipientEntity` permission to read the model that we've encrypted. Run the cell below to see for yourself.\n",
    "\n",
    "Once we deploy the model, we will create a new application called `pong-{model_name}` and link the model to that application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_model(clipper_conn, tag, model_loc, model_version=1):\n",
    "    model_name = \"pong-policy-\" + tag\n",
    "    app_name = \"pong-\" + tag\n",
    "    \n",
    "    # load the encrypted model into memory\n",
    "    with open(model_loc, 'rb') as f:\n",
    "        ciphered_model = f.read()\n",
    "\n",
    "    auth_deploy_python_model(clipper_conn, model_name,wave,\n",
    "        recipientEntity, ciphered_model, version=1, input_type=\"doubles\")\n",
    "\n",
    "    clipper_conn.register_application(name=app_name, default_output=\"0\", input_type=\"doubles\", slo_micros=100000)\n",
    "    clipper_conn.link_model_to_app(app_name=app_name, model_name=model_name)\n",
    "\n",
    "deploy_model(clipper_conn, model_tag.v, model_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're going to go ahead and use the helper function defined above to grant permission to the recipient entity to decrypt the ciphered model. Run the cell below to grant decryption permission and deploy the model to Clipper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grantPermission(orgEntity, recipientEntity, 'models/pong/' + model_tag.v)\n",
    "deploy_model(clipper_conn, model_tag.v, model_location, model_version=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for comparison's sake, we're going to train another imitation learning model with a different dataset -- `imitation-large.csv`. This dataset incorporates training data from two different players, and it should ideally perform better than the previous version. \n",
    "\n",
    "Run the cell below to use Flor's update features to change the value of the `model_tag` literal and the imitation data artifacts. It will then recreate the encrypted model with the new data and grant permission to the WAVE entity to access it. Finally, it will deploy the model using the function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a new data file\n",
    "DATA_FILE = 'imitation-large.csv'\n",
    "\n",
    "# update the imitation_data artifact to point at the new file\n",
    "imitation_data.loc = DATA_FILE\n",
    "\n",
    "# update the model tag literal based on this new fine\n",
    "model_tag.v = DATA_FILE.split('.')[0].split('-')[1]\n",
    "\n",
    "# train the model on the new data file2\n",
    "encrypted_model.pull(utag=model_tag.v)\n",
    "model_location = encrypted_model.resolve_location()[0]\n",
    "\n",
    "# grant decryption permissions for the new model\n",
    "grantPermission(orgEntity, recipientEntity, 'models/pong/' + model_tag.v)\n",
    "\n",
    "# deploy the new model\n",
    "deploy_model(clipper_conn, model_tag.v, model_location, model_version=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to start the pong server locally, and you'll be given a URL which you can open in your browser to play against the two pong models you've just trained. You'll select the models as opponents in the dropdown menu on the left hand side. You'll notice there's a reinforcement learning option there as well -- we'll come back to that model soon. For now, you should just play a game against the `Imitation Learning (Small)` and `Imitation Learning (Large)` models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipper_addr = clipper_conn.get_query_addr()\n",
    "\n",
    "import subprocess\n",
    "server_handle = subprocess.Popen([\"./start_webserver.sh\", clipper_addr], stdout=subprocess.PIPE)\n",
    "print(str(server_handle.stdout.readline().strip(), 'utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that, surprisingly, the model trained on the large dataset performs significantly worse than the model trained on the small dataset. Let's take a look at the difference between the two and see why:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the two CSVs into dataframes\n",
    "import pandas as pd\n",
    "large = pd.read_csv('imitation-large.csv')\n",
    "small = pd.read_csv('imitation-small.csv')\n",
    "\n",
    "# complicated Pandas syntax to diff the two datasets\n",
    "large[large.merge(small, on=list(large.columns), how='left', indicator=True)['_merge'] == 'left_only']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that all the entries in the large dataset have been made by Ion, who apparently isn't very good at Pong. As a result, the noise from these entries worsens the performance of the model in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll train an RL policy to play pong. We will use Proximal Policy Optimization (PPO) to train this agent. We'll see that the RL policy trained here will perform significantly better than either of the imtiation learning models we trained above. As above, we'll first define a couple Flor functions. We will also reuse the encrypt model function defined above.\n",
    "\n",
    "First, we need to either start Ray or make sure it's already running on our machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flor.func\n",
    "def start_ray(**kwargs):\n",
    "    try:\n",
    "        ray.get([])\n",
    "    except:\n",
    "        ray.init()    \n",
    "    return {'exit_code': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a function called `train_agent` that takes an environment configuration and trains the model for a specified number of iterations. Both the environment config and the number of iterations will be Flor literals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flor.func\n",
    "def train_agent(env_config, num_iterations, model, **kwargs):\n",
    "    import cloudpickle as cp\n",
    "    \n",
    "    # create an RL lib environment and register it\n",
    "    register_env(\"pong_env\", lambda ec: pong_py.PongJSEnv())\n",
    "    agent = ppo.PPOAgent(env=\"pong_env\", config={\"env_config\": {}})\n",
    "\n",
    "    # train for the specified number of iterations\n",
    "    for i in range(num_iterations):\n",
    "        result = agent.train()\n",
    "        \n",
    "    # save the model to a file\n",
    "    with open(model, 'wb') as f:\n",
    "        f.write(agent.save_to_object())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll string all of these functions together into another Flor experiment called `rl-pong`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tag = 'rl'\n",
    "\n",
    "with flor.Experiment('rl-pong') as ex:\n",
    "    # make sure that Ray is running before attempting to train a model\n",
    "    do_start_ray = ex.action(start_ray, [])\n",
    "    exit_code = ex.literal(name='exit_code', parent=do_start_ray)\n",
    "    \n",
    "    # define configurations variables relevant to training the RL model\n",
    "    env_config = ex.literal({}, 'env_config') # TODO: Fill env_config\n",
    "    num_iterations = ex.literal(50, 'num_iterations')\n",
    "\n",
    "    # setup the training action and the save location of the checkpoint\n",
    "    do_train_agent = ex.action(train_agent, [env_config, num_iterations, exit_code])\n",
    "    model = ex.artifact('model.pkl', 'model', do_train_agent)\n",
    "    \n",
    "    model_tag = ex.literal(name='model_tag', v=model_tag)\n",
    "    \n",
    "    # serialize the wave entity, so we can track it as an artifact\n",
    "    serializeEntity(orgEntity, ENTITY_FILE)\n",
    "    granting_entity = ex.artifact(ENTITY_FILE, 'granting_entity')\n",
    "    \n",
    "    do_encrypt_model = ex.action(encrypt_model, [granting_entity, model, model_tag])\n",
    "    encrypted_model = ex.artifact('encrypted_model.bin', 'encrypted_model', do_encrypt_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll plot the new experiment graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encrypted_model.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll run this pipeline in the usual fashion. Because we're training for 50 iterations, this might take a few minutes to run to completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encrypted_model.pull()\n",
    "model_location = encrypted_model.resolve_location()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we have to grant permission to the entity to decrypt the model, but this time, we'll use a wildcard, letting it decrypt anything in the `models/pong/` resource space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grantPermission(orgEntity, recipientEntity, 'models/pong/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And deploy the model to Clipper under a new application name (`pong-rl`). Note that this code is the same as the cell above but is repeated for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clipper_util.auth_deployer import auth_deploy_rllib_model\n",
    "\n",
    "# load the encrypted model into memory\n",
    "with open(model_location, 'rb') as f:\n",
    "    ciphered_model = f.read()\n",
    "\n",
    "model_name = \"pong-policy-\" + model_tag.v\n",
    "app_name = \"pong-\" + model_tag.v\n",
    "\n",
    "def predict(model, states):\n",
    "    return [str(model.compute_action(s)) for s in states]\n",
    "\n",
    "auth_deploy_rllib_model(\n",
    "    clipper_conn,\n",
    "    model_name,\n",
    "    predict,\n",
    "    wave,\n",
    "    recipientEntity,\n",
    "    ciphered_model,    \n",
    "    version=1,\n",
    "    input_type=\"doubles\"\n",
    ")\n",
    "\n",
    "clipper_conn.register_application(name=app_name, default_output=\"0\", input_type=\"doubles\", slo_micros=100000)\n",
    "clipper_conn.link_model_to_app(app_name=app_name, model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can go back to the Pong tab and play against the RL agent we just deployed. We expected the RL agent to be much better than either of the imitation learning models, but the difference might not be discernible in a single game. We're aggregating the reuslts from all the players in the room to see which model has the best point differential. The graph on the projector is showing `points_scored - points_conceded` for each of the three models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we:\n",
    "\n",
    "* trained 3 ML models: two using imitation learning, and one using reinforcement learning via Ray and RLlib \n",
    "* captured every one of these model training pipeline using Flor\n",
    "* used Flor's tracking to diff the datasets and understand why one performed better than the other. \n",
    "* encrypted each model with WAVE and deployed them into Clipper\n",
    "\n",
    "We hope you had a great RISE Camp and look forward to seeing you again next year!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
