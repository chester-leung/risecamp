{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Motivation\n",
    "************************************************************************************\n",
    "\n",
    "In the era of ‘big data’, vast amounts of data are being collected and curated in the form of arrays across the social, physical, engineering, biological, and ecological sciences.  Analysis of the data relies on a variety of matrix decomposition methods which seek to exploit low-rank features exhibited by the high-dimensional data.  Indeed, matrix decompositions are often the workhorse algorithms for scientific computing applications in the areas of applied mathematics, statistical  computing,  and  machine  learning.   Despite  our  ever-increasing  computational power, the emergence of large-scale datasets has severely challenged our ability to analyze data  using  traditional  matrix  algorithms.   Moreover,  the  growth  of  data  collection  is  far\n",
    "outstripping computational performance gains.  The computationally expensive singular value\n",
    "decomposition  (SVD)  is  the  most  ubiquitous  method  for  dimensionality  reduction,  data\n",
    "processing and compression.  **The concept of randomness has recently been demonstrated as\n",
    "an effective strategy to easing the computational demands of low-rank approximations** such as the SVD, thus allowing for a scalable architecture for modern\n",
    "‘big data’ applications. Here is the computational performance for a rank $500$ approximation using a AWS EC2 C5 instance with 36 CPUs:\n",
    "\n",
    "<img src=\"img/c48xlarge.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "**Randomness as a computational strategy is interesting if we an make the following assumption:  the data matrix to be approximated has low-rank structure, i.e., the rank is smaller than the ambient\n",
    "dimension of the measurement space.** Fortunately, this assumption is valid for most machine learning applications.\n",
    "\n",
    "#  Tutorial Outcomes\n",
    "************************************************************************************\n",
    "\n",
    "* Quick refresher about the singular value decomposition (SVD). \n",
    "\n",
    "* Learn about randomized numerical linear algebra (RandNLA) for data science.\n",
    "\n",
    "* Learn how to use the randomized svd function from the ristretto package.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "* You will use the randomized SVD for image compression. \n",
    "\n",
    "* You will use the randomized SVD to implement a fast robust PCA algorithm to remove outliers from a dataset. \n",
    "\n",
    "* You will use the blocked randomized SVD to scale applications in light of limited fast memory.\n",
    "\n",
    "* (Optional) If you like, you can try to use randomized methods to extract features from the EMNIST dataset to train a predictive model.\n",
    "\n",
    "* (Optional) If you like, you can try to implement the randomized SVD at the end of this tutorial using just a few lines of code.\n",
    "\n",
    "#  Brief Review of the Singular Value Decomposition (SVD)\n",
    "************************************************************************************\n",
    "The SVD provides a numerically stable matrix decomposition that can be used to obtain\n",
    "low-rank approximations, to compute the pseudo-inverses of non-square matrices, and to find\n",
    "the least-squares and minimum norm solutions of a linear model.  Further, the SVD is the\n",
    "workhorse algorithm behind many machine learning concepts, for instance, matrix completion,\n",
    "sparse coding,  dictionary learning,  PCA and robust PCA. \n",
    "\n",
    "Suppose we are given a $m\\times n$ matrix $\\mathbf{A}$, where $m$ denotes the number of rows and $n$ the number of columns. Without loss of generality we assume that $n\\leq m$. The singular value decomposition admits the following factorization \n",
    "\n",
    "\\begin{equation}\n",
    "\t\\begin{array}{cccccccccc}\t\t\t\n",
    "\t\\mathbf{A}  & \\approx & \\mathbf{U} & \\mathbf{\\Sigma}  & \\mathbf{V}^\\top\\\\\n",
    "\tm\\times n &   &  m\\times n & n\\times n  & n\\times n\n",
    "\t\\end{array} \n",
    "\\end{equation}\n",
    "\n",
    "$\\mathbf{U}$ denotes the left singular vectors, $\\mathbf{V}$ are the right singular, and $\\mathbf{\\Sigma}$ contains the non-negative singular values vectors. In many applications the numeric rank $r$ of a matrix is much smaller than the ambient dimensions of the input matrix \n",
    "\n",
    "\n",
    "<img src=\"img/svd.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "This is, for instance, the case if some of the information in the data are redundant.\n",
    "In such situations we are interested in computing as few components as possible. This leads to the concept of low-rank factorizations, where we try to represent the original data matrix by using as few singular vectors as possible. Often, we define a desired target-rank $k$ to yield the following factorization\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\begin{array}{cccccccccc}\t\t\t\n",
    "\t\\mathbf{A}  & \\approx & \\mathbf{U} & \\mathbf{\\Sigma}  & \\mathbf{V}^\\top\\\\\n",
    "\tm\\times n &   &  m\\times k & k\\times k  & k\\times n\n",
    "\t\\end{array} \n",
    "\\end{equation}\n",
    "\n",
    "Choosing  an  optimal  target  rank $k$ is  highly  dependent  on  the  task.   One  can  either  be interested in a highly accurate reconstruction of the original data, or in a very low dimensional representation of dominant features in the data.  In the former case $k$ should be chosen close to the effective rank, while in the latter case $k$ might be chosen to be much smaller.\n",
    "\n",
    "##  Randomized Singular Value Decomposition (RSVD)\n",
    "************************************************************************************\n",
    "**Randomized algorithms have been recently popularized, in large part due to their ‘surprising’\n",
    "reliability and computational efficiency** to obtain an approximate rank-$k\n",
    "$ factorization. When the dimensions of $\\mathbf{A}$ are large, randomized methods are more efficient than truncating the full SVD. We omit technical details here, but refer the interested reader to this neat blog post by Andrew Tulloch (Facebook Reserach):\n",
    "\n",
    "* https://research.fb.com/fast-randomized-svd/\n",
    "\n",
    "For a more technical treatment we refer to the following references:\n",
    "\n",
    "* Randomized algorithms for matrices and data (https://arxiv.org/abs/1104.5557)\n",
    "* Finding structure with randomness (https://arxiv.org/abs/0909.4061)\n",
    "* Randomized Matrix Decompositions using R (https://arxiv.org/pdf/1608.02148.pdf)\n",
    "\n",
    "# The Ristretto Package\n",
    "You can find randomized methods for linear algebra in scikit learn to compute the SVD and PCA. Here, we use the ristretto package, which provides a collection of randomized algorithms for a range of applications. This package is still under development, and many more features are to come soon. Just have a look to:\n",
    "\n",
    "* https://github.com/erichson/ristretto/\n",
    "* https://ristretto.readthedocs.io/en/latest/\n",
    "\n",
    "\n",
    "You don't have to worry about it now, but installation is easy:\n",
    "\n",
    "``` pip install git+https://github.com/erichson/ristretto/```\n",
    "\n",
    "### Contact\n",
    "\n",
    "Hints, suggestions and feedback are warmly welcome, just drop an email to Ben ( erichson@berkeley.edu ) or Joe (joseph.edward.knox@gmail.com), or use the wish list on GitHub ( https://github.com/erichson/ristretto/projects ). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the libraries we are going to use for this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sci\n",
    "from scipy import ndimage \n",
    "from tqdm import trange\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set(style=\"white\")\n",
    "from misc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most importantly, we are using the following two functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import svd\n",
    "from ristretto.svd import compute_rsvd as rsvd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by grabbing a test image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in image\n",
    "A = plt.imread('/mnt/data/lion.jpg')\n",
    "\n",
    "# Convert to a grayscale image\n",
    "A = np.dot(A[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "# Display image\n",
    "fig = plt.figure(figsize=(11, 9))\n",
    "plt.imshow(A, cmap = 'gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the truncated SVD using Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we define a new function ```tsvd()``` to compute the truncated SVD using Numpy's svd function. In the following we can then pass the desired target-rank as an argument so that the function returns the truncated singular vectors and values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsvd(A, rank):\n",
    "    U, s, Vt = np.linalg.svd(A , full_matrices=False)\n",
    "    return U[:,0:rank], s[0:rank], Vt[0:rank,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compute the truncated SVD to compress the above image. For instance, we can use the target-rank 200:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "U, S, Vt = tsvd(A, rank=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reconstruct the original image as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_appox = (U*S).dot(Vt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relative error is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Relative reconstruction error:', np.linalg.norm(A-A_appox) / np.linalg.norm(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we get a nearly as good result, but faster? Let's try the randomized SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ristretto.svd import compute_rsvd as rsvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "U, S, Vt = rsvd(A, rank=200, oversample=20, n_subspace=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The randomized SVD is quite a bit faster, but how good did we do in terms of the approximation quality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_rand_appox = (U*S).dot(Vt)\n",
    "print('Relative reconstruction error:', np.linalg.norm(A-A_rand_appox) / np.linalg.norm(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not good enough? \n",
    "* Well, go back and try to change the oversample parameter value to 50. \n",
    "* If, you think that this is still not good enough, then you can increase the number of subspace iterations, say, to 6. \n",
    "\n",
    "You will see that computing the randomized SVD requires some more computational resources, now. In summary, **you can control the trade-off between speed and accuracy via the amount of oversampling and the number of subspace iterations**. In most machine learning applications the default values (oversample=20, n_subspace=2) are good enough, in our experience. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do a visual comparison between the deterministic and the randomized low-rank approximations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image\n",
    "f = plt.figure(figsize=(22,9))\n",
    "f.add_subplot(121)\n",
    "plt.imshow(A_appox, cmap = 'gray')\n",
    "plt.axis('off')\n",
    "plt.title('Deterministic SVD', fontsize=22)\n",
    "\n",
    "f.add_subplot(122)\n",
    "plt.imshow(A_rand_appox, cmap = 'gray')\n",
    "plt.axis('off')\n",
    "plt.title('Randomized SVD', fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrupted Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some salt and pepper noise to our image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Anoisy = noisy(\"s&p\", A, amount=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image\n",
    "fig = plt.figure(figsize=(11, 9))\n",
    "plt.imshow(Anoisy, cmap = 'gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we see that both the determenistic and randomized algorithms provide a grainy approximation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vt = tsvd(Anoisy, rank=200)\n",
    "print('Relative reconstruction error:', np.linalg.norm(A-(U*S).dot(Vt)) / np.linalg.norm(A) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vt = rsvd(Anoisy, rank=200)\n",
    "print('Relative reconstruction error:',np.linalg.norm(A-(U*S).dot(Vt)) / np.linalg.norm(A) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, the randomized SVD does slightly better. This is what we call an intrinsic regularization effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image\n",
    "fig = plt.figure(figsize=(11, 9))\n",
    "plt.imshow((U*S).dot(Vt), cmap = 'gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many data science applications we need robust methods which can more effectively account for corrupt or missing data.  Indeed, outlier rejection is critical in many applications as data is rarely free of corrupt elements.\n",
    "Robustification methods decompose data matrices as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\begin{array}{cccccccccc}\t\t\t\n",
    "\t\\mathbf{A}  & \\approx & \\mathbf{L} & \\mathbf{E}\\\\\n",
    "\tm\\times n &   &  m\\times n & m\\times n \n",
    "\t\\end{array} \n",
    "\\end{equation}\n",
    "where $\\mathbf{L}$ denotes a low-rank matrix and $\\mathbf{E}$ a sparse matrix capturing the corrupted entries (outliers) of the input data matrix.\n",
    "\n",
    "This  form of additive decomposition (i.e., the  decomposition  of  a  matrix  into  its  sparse  and  low-rank  components) is also denoted as robust principal component analysis (RPCA). Its remarkable ability to separate high-dimensional matrices into low-rank and sparse component makes RPCA an invaluable tool for data science. However, the biggest challenge for robust PCA is computational efficiency, especially given the iterative\n",
    "nature of the optimization required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the robust PCA is based on an iterative scheme, requiring to computing the SVD in each iteration. The computational costs can be drastic using the deterministic SVD. In the following we have compiled some toy code to compute the robust PCA. Note, that the following code is just intended to demonstrate the idea. More concretely, we use a proximal gradient algorithm to solve the following minimixation problem:\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\text{min}  \\, \\|\\mathbf{A} -  \\mathbf{L} - \\mathbf{E}\\|_F^2 \\, + \\, \\alpha \\|\\mathbf{E}\\|_1,\n",
    "\\end{equation}\n",
    "where $\\alpha$ controls the level of sparsity of $\\mathbf{E}$. If we would know the outlier matrix $\\mathbf{E}$, then finding $\\mathbf{L}$ has closed form solution, i.e., the best possible rank-k matrix $\\mathbf{L}$ minimizing the above problem is given by the truncated SVD of $(\\mathbf{A} - \\mathbf{E})$. However, since we do not know $\\mathbf{E}$, we need to alternatingly update $\\mathbf{L}$ and $\\mathbf{E}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_pca(A, rank, alpha, max_iter):\n",
    "    \n",
    "    # Init\n",
    "    E = np.zeros_like(A) \n",
    "    alpha *= np.linalg.norm(A, 2)**2 \n",
    "    \n",
    "    for i in trange(max_iter):\n",
    "        \n",
    "        # i) Compute the SVD\n",
    "        U, s, Vt = tsvd(A-E, rank=rank)\n",
    "        \n",
    "        # ii) Compute prox gradient step\n",
    "        grad = E - (A - (U*s).dot(Vt)) # compute gradient\n",
    "        E = E - 0.1 * grad\n",
    "        \n",
    "        # iii) Soft threshold (l1 regularization)\n",
    "        E = np.clip(abs(E) - alpha * 0.1, 0, None) * np.sign(E)    \n",
    "        \n",
    "        \n",
    "    return (U*s).dot(Vt), E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's run the algorithm using the truncated SVD for a maximum of 20 iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Are, E = robust_pca(Anoisy, rank=200, alpha=1e-9, max_iter=20)\n",
    "print('Relative reconstruction error:', np.linalg.norm(A-Are) / np.linalg.norm(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes quite some time!!! \n",
    "\n",
    "* First, can you change the code and use the ```rsvd()``` function instead? Let's rerun the code.\n",
    "\n",
    "Now, this should be a bit faster, right? \n",
    "\n",
    "* Next, we can increase the number of max iteration, say, to 50. This should improve the reconstruction error quite a bit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image\n",
    "f = plt.figure(figsize=(22,9))\n",
    "f.add_subplot(121)\n",
    "plt.imshow(Are, cmap = 'gray')\n",
    "plt.axis('off')\n",
    "plt.title('Reconstructed Image', fontsize=22)\n",
    "\n",
    "f.add_subplot(122)\n",
    "plt.imshow(E, cmap = 'gray')\n",
    "plt.axis('off')\n",
    "plt.title('Removed Outliers', fontsize=22)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling the SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use the SVD to extract features from high-resolution sea surface temperature (SST) data.  The SST data are widely studied in climate science for climate monitoring and prediction, providing an improved understanding\n",
    "of the interactions between the ocean and the atmosphere. Specifically, the daily SST measurements are constructed by combining infrared satellite data with observations provided by ships and buoys.  In order to account and compensate for platform differences and sensor errors, a bias-adjusted methodology is used to combine the measurements from the different sources.  Finally, the spatially complete SST map is produced via interpolation. The data are provided by the National Oceanic and Atmospheric Administration (NOAA) via their web site at https://www.esrl.noaa.gov/psd/.  \n",
    "\n",
    "Data are available for the years from 1981 to 2018 with a temporal resolution of 1 day and a spatial grid resolution of $0.25^{\\circ}$.  In total, the data consists of $m = 13,149$ temporal snapshots which measure the daily temperature\n",
    "at $1440×720 = 1,036,800$ spatial grid points.  Since we omit data over land, the ambient dimension reduces to\n",
    "$n= 691,150$ spatial measurements in our analysis. Concatenating the reduced data yield a 36GB data matrix of dimension $691,150×13,149$, which is sufficiently large to test scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are stored in a hdf5 file and are to big to fit into our fast memory. However, the hdf5 data format allows us to efficentlys access parts of the data, i.e., we can read in some columns or some rows. We start by creating a new h5py object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(\"/mnt/data/sstHD.hdf5\", \"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can visualize a couple of the snapshots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frames(f['sstHD'][:,[0,1000,2000,3000,4000,5000]], x=2, y=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to use the SVD provided by Numpy or Scipy will result into a memory error (you can try it yourself):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#U, s, Vt = tsvd(f['sstHD'], rank=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The randomized SVD function provided by the ristretto package allows us to compute the SVD using a blocked scheme. You can control the number of blocks via the argument ```n_blocks```. This will take about 2 minutes, so that is a good point to grab a cup of coffee. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "U, s, Vt = rsvd(f['sstHD'], rank=200, n_blocks=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can plot the first six extracted modes (left singular vectors). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frames(U, x=2, y=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower middle plot identifies the intermittent El Niño and La Niña warming events, which are famously implicated in global weather patterns and climate change. The El Niño Southern Oscillation (ENSO) is defined as any sustained temperature anomaly above running mean temperature with a duration of 9 to 24 months. The canonical El Niño is associated with a narrow band of warm water off coastal Peru that is commonly referred as NIÑO 1+2, 3, 3.4, or 4 to differentiate the types of bands. This mode is often used to compute the canonical Oceanic Niño Index (ONI)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blocked Randomized SVD\n",
    "\n",
    "Here, a sequential scheme is used to compute the blocked randomized SVD. Thus, if the data fit into fast memory the performance of the standard randomized SVD is better:\n",
    "\n",
    "<img src=\"img/rand_only_c48xlarge.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "However, the advantage of the blocked scheme is the reduced memory requirement. Given a distributed computing envirorment, we could perform the computations in parallel. More on parallel and distributed computing follows in Tutorial 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/mlhO9.jpg\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMNIST Classification  using Randomized PCA and Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Extended MNIST (EMNIST) is a dataset that poses a more challenging\n",
    "classification tasks than MNIST. More concretely, it involves both letters  and  digits. For more details have a look here:\n",
    "* EMNIST: an extension of MNIST to handwritten letters: https://arxiv.org/pdf/1702.05373v1.pdf [1]\n",
    "* NIST: https://www.nist.gov/itl/iad/image-group/emnist-dataset\n",
    "\n",
    "Motivation for the following section is the question if we can build a simple predictive model which is compareable with the the accuracy ($~78\\%$) of the classification results report in [1]. However, instead of using a NN with a large number of hidden layers, we use some \"old school\" ML techniques. So, let's see how good we can do within 5 minutes? Here is the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input, training_targets, testing_input, testing_targets, training_labels, testing_labels = load_emnist('/mnt/data/emnist-balanced.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset comprises 47 classes, 112800 instances for training and 18800 instances for testing. Each instance is a 28×28 pixel images, meaning that they are 784-dimensional if we consider each pixel as a variable. Here are some samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_emnist(training_input, training_labels, 6, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the principal components via the SVD we need to mean center the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_mean = np.mean(training_input, axis=0)\n",
    "\n",
    "training_centered = training_input - Xtrain_mean\n",
    "testing_centered = testing_input - Xtrain_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can simply use the rsvd routine from the ristretto library to compute the top $60$ principal directions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_, _, A = tsvd(training_centered, rank=60)  # project from 784 to 2 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the extracted principal directions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_components(A.T, 4, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we project the data to low-dimensional space, i.e., compute the new principal components which we will use as  new features for classification later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_pca_training = training_centered.dot(A.T)\n",
    "Z_pca_testing = testing_centered.dot(A.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you like sci-kit learn better, feel free to use the following couple of lines. Note, that sci-kit learn's PCA function uses the randomized svd, too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#from sklearn.decomposition import PCA\n",
    "#pca = PCA(n_components=50)\n",
    "#pca.fit(training_centered)\n",
    "#Z_pca_training = pca.transform(training_centered)\n",
    "#Z_pca_testing = pca.transform(testing_centered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to use the random forest technique (https://en.wikipedia.org/wiki/Random_forest) for classification. Specifically, we use the algorithm implemented in the scikit-learn package (http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). For simplicity we just use the default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RF = RandomForestClassifier(n_estimators=50, random_state=123)\n",
    "model_RF.fit(Z_pca_training, training_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the classification summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = model_RF.predict(Z_pca_testing)\n",
    "print(metrics.classification_report(ypred, testing_targets, target_names=testing_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! We did good, didn't we? Well, can we do better by spending another 5 minutes on this task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Features for Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we explore how good sparse features will do as input for our predictive model. We will use the sparse PCA function of the ristretto package to compute the sparse features. Don't worry if you haven't used sparse PCA before, the details do not matter too much for the moment. Again, we will compute $50$ sparse features, where we control the level of sparsity with the tuning parameter $\\alpha$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ristretto.pca import compute_rspca as rspca\n",
    "B, _, _, _ = rspca(training_centered, n_components=60, alpha=1e-3, beta=1e-8, max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the computed sparse weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_components(B, 4, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We project the data to low-dimensional space using the sparse weights to extract new features from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_spca_training = training_centered.dot(B)\n",
    "Z_spca_testing = testing_centered.dot(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's feed the new features into our learning machine and see how good we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RF = RandomForestClassifier(n_estimators=50, random_state=123)\n",
    "model_RF.fit(Z_spca_training, training_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict classes\n",
    "ypred_sparse = model_RF.predict(Z_spca_testing)\n",
    "print(metrics.classification_report(ypred_sparse, testing_targets, target_names=testing_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice improvement!? :) \n",
    "* Here is our latest paper on sparse PCA: https://www.researchgate.net/publication/324167266_Sparse_Principal_Component_Analysis_via_Variable_Projection or https://arxiv.org/abs/1804.00341"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some more Insights about the Randomized SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ristretto package advocates a two-stage probabilistic framework to compute a near-optimal low-rank approximation.  Conceptually, this framework splits the computation into the two logical steps:\n",
    "\n",
    "* $\\textbf{Stage A:}$ Construct a low dimensional subspace that approximates the range of $\\mathbf{A} \\in \\mathbb{R}^{m\\times n}$. Specifically, it is the aim to find a matrix $\\mathbf{Q} \\in \\mathbb{R}^{m\\times k}$ with orthonormal columns, where $k$ denotes the target rank, such that $\\mathbf{A} \\approx \\mathbf{Q}\\mathbf{Q}^\\intercal\\mathbf{A}$ is satisfied. \n",
    "\n",
    "* $\\textbf{Stage B:}$ Form a smaller matrix $\\mathbf{B} \\in \\mathbb{R}^{k\\times n}$ by restricting the high-dimensional input matrix to the low-dimensional space spanned by the near-optimal basis $\\mathbf{Q}$. This smaller matrix can then be used to compute a desired low-rank approximation.\n",
    "\n",
    "The first computational stage is where randomness comes into the play, while the second stage\n",
    "is purely deterministic.  In the following, the two stages are described in more detail. Pictorially, here is what we do:\n",
    "\n",
    "\n",
    "<img src=\"img/stageA.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage A\n",
    "***********"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, the first step is to generate a random test matrix $\\mathbf{\\Omega}$ of dimension $n \\times l$ were $l=k+p$. Here, we need to define a target rank $k$, i.e, we need to decide how many dominant singular vectors and values we aim to approximate. Further, to improve the approximation accuracy, we introduce some slight oversampling in addition, here we use $p=20$. We will use the image which we have used at the very beginning of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = plt.imread('./data/lion.jpg') # Read image\n",
    "A = np.dot(A[...,:3], [0.299, 0.587, 0.114]) # Convert to a grayscale image\n",
    "m, n = A.shape # Get shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by drawing the entries of $\\mathbf{\\Omega}$ from the standard normal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k, p = 200, 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Omega = sci.random.standard_normal((n, (k+p))) # standard normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute a sketch matrix $\\mathbf{Y}$, i.e., we sketch the column space of the input matrix (image) $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = A.dot(Omega)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the accuracy of the basis, which we construct next, we can perform additional power iterations. We leave this as an exercise (skip this for the moment). If you need some guidance, have a look to https://arxiv.org/pdf/1608.02148.pdf on page 9 to Algorithm 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 2 # number of (optional) power iterations\n",
    "\n",
    "#for i in range(q):\n",
    "    # compute QR decomposition of Y\n",
    "    # compute QR decomposition of A.T.dot(Q) \n",
    "    # update samples matrix Y as Y = A.dot(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{Y}$ forms a basis for the column space of the input matrix, we can use the 'economic' QR decomposition to form an orthonormal basis, so that $\\mathbf{Q}^\\top\\mathbf{Q}=\\mathbf{I}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q , _ = sci.linalg.qr(Y ,  mode='economic' ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage B\n",
    "************\n",
    "Now, having $\\mathbf{Q}$, we aim to find a smaller $l \\times n$ matrix $\\mathbf{B}$. We proceed by projecting the high-dimensional input matrix $\\mathbf{A}$ to low-dimensional space as \n",
    "\\begin{equation}\n",
    "\\mathbf{B} = \\mathbf{Q}^\\top\\mathbf{A}.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = Q.T.dot(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the approximate SVD is computed using a standard (deterministic) algorithm so that we attain the following decomposition\n",
    "\\begin{equation} \n",
    "\\mathbf{B} = \\mathbf{\\tilde{U}} \\mathbf{\\Sigma}\\mathbf{V}^\\top.\t\n",
    "\\end{equation} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Utilde, s , Vt = np.linalg.svd( B , full_matrices=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It remains to recover the high-dimensional left singular vectors. Recall that\n",
    "\t$\\mathbf{A} \\approx  \\mathbf{Q}{\\mathbf{Q^{\\top}}\\mathbf{A}} = \\mathbf{Q} {\\mathbf{B}} = \\mathbf{Q}\\mathbf{\\tilde{U}} \\mathbf{\\Sigma}\\mathbf{V^{\\top}}= \\mathbf{U} \\mathbf{\\Sigma}\\mathbf{V^{\\top}}$. Hence, $\\mathbf{\\tilde{U}}:= \\mathbf{Q}\\mathbf{\\tilde{U}}$.\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = Q.dot(Utilde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how good the approximation quality is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_rsvd = (U*s).dot(Vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image\n",
    "fig = plt.figure(figsize=(15, 11))\n",
    "plt.imshow(A_rsvd, cmap = 'gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the relative reconstruction error to quantify the performance of the randomized algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print relative error\n",
    "print('Relative error:', sci.linalg.norm(A-A_rsvd)/sci.linalg.norm(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try it with two additional power iterations / subspace iterations and see if you can improve the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/end.jpeg\" alt=\"Drawing\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
